\chapter{\qWhy{} and \qHow{}}
\label{cha:intro}

\begin{note}
  Our interest is understanding the way an event in which an agent concludes happens.

  \begin{scenario}[Multiplication]%
    \label{illu:gist:calc}%
    An agent enters `\gistCalcLHS{}' into a calculator and presses a button marked `\(=\)'.
    The calculator displays `\gistCalcRHS{}'.

    The agent pauses for a moment.
    They have a good understanding of arithmetic.
    And, given the display on the calculator, it follows that if the calculator is trustworthy, then they would not fail to conclude \propM{\gistCalcEq{}} via their understanding of arithmetic.

    The agent concludes \propM{\gistCalcEq{}}.
  \end{scenario}

  \noindent%
  Intuitively, the agent concludes \propM{\gistCalcEq{}} from the calculator.
  More could be said about the way the agent concludes but these details won't be of too much interest.%
  \footnote{
    For example, classify the agent as regarding the calculator as a source of testimony, adding that testimony are factive, and so concluded \propM{\gistCalcEq{}} from the testimony of the calculator.
  }

  The agent's understanding of arithmetic, it seems, had no significant role.
  So long as the calculator was correct, the agent had the opportunity to conclude \propM{\gistCalcEq{}} by their understanding of arithmetic, but the agent did not perform any arithmetic.
\end{note}

\begin{note}
  Abstracting a little, \propM{\gistCalcEq{}} corresponds to a state of affairs.
  If the truths of mathematics are necessary, then this state of affairs never fails to be, but \propM{\gistCalcEq{}} captures the way things are in the same way as \propI{Kangaroos have tails} captures the way things are.
  For ease we refer to states of affairs as propositions.

  Continuing the abstraction, when the agent concludes \propM{\gistCalcEq{}}, we may say the agent concludes \propM{\gistCalcEq{}} has value \valI{True}.
  Even if \propM{\gistCalcEq{}} never fails to be, the calculator may have been faulty and the agent may have concluded \propM{\gistCalcEqBad{}} has value \valI{True} or \propM{\gistCalcEq{}} has value \valI{False}.

  Likewise, as the agent concluded \propM{\gistCalcEq{}} has value \valI{True} from the calculator, we may say the agent concluded \propM{\gistCalcEq{}} has value \valI{True} from some \pool{} of premises.
\end{note}

\begin{note}
  Abstracting from almost all the details of \autoref{illu:gist:calc} then, we say an agent concluded some \prop{} has some \val{} from some \pool{} of premises.%
  \footnote{
    For ease, we abbreviate `\pool{} of premises' to `\pool{}'.
  }

  The `\emph{from}' in the previous statement seems to do work.%
  \footnote{
    Or any other synonym.
    E.g. the conclusion being `a result of', `due to', `because of', \dots the relevant \pool{}.
  }
  Captures something beyond the observation that there was a sequence of events in which the agent used the calculator and then concluded \propM{\gistCalcEq{}} has value \valI{True}.

  Our understanding of \autoref{illu:gist:calc} is be different if the agent concluded \propM{\gistCalcEq{}} has value \valI{True} from the their understanding of arithmetic, or if the agent concluded \propM{\gistCalcEq{}} has value \valI{True} from some other person.

  Alternatively we may say something about the calculator \emph{supported} \propM{\gistCalcEq{}} being \valI{True}, from the \agpe{}.
  Speaking in terms of an \agents{} conclusion `from' some \pool{} is cosy, but speaking in terms of \ros{}  allows abstraction from an event in which an agent concludes.

  We make to basic assumptions about \ros{}:
  \begin{enumerate}
  \item
    When an agent concludes \(\phi\) has value \(v\) from \(\Phi\) a \ros{} between \(\phi\), \(v\) and \(\Phi\) holds, from the \agpe{}
  \item
    A \ros{} between \(\phi\), \(v\) and \(\Phi\) may hold, from an \agpe{}, without a prior event in which agent concludes \(\phi\) has value \(v\) from \(\Phi\).
  \end{enumerate}
\end{note}

\begin{note}
  So, our interest is understanding the way an event in which some agent concludes some proposition \(\phi\) has some value \(v\) from some \pool{} \(\Phi\) happens.
  And, it seems \ros{1} provide some understanding of the way an event in which some agent concludes happens.

  Specifically, a \ros{} seems to provide some understanding of \emph{why} an event in which some agent concludes happens.
  A \ros{} does not clearly provide some understanding of \emph{how} the event in which the agent concludes happens.

  Still, there is a plausible connexion between `why' and `how' with respect to \autoref{illu:gist:calc}.
  For, the agent concludes \propM{\gistCalcEq{}} has value \valI{True} from some \pool{}, and hence the agent has a `\wit{}' for the relevant \ros{}.
\end{note}

\begin{note}
  Here's a second, slightly more complex \scen{0}:

  \begin{scenario}[Animalism]%
    \label{scen:animalism}%
    `Four legs good, two legs bad.'
    This, he said, contained the essential principle of Animalism.
    Whoever had thoroughly grasped it would be safe from human influences.
    The birds at first objected, since it seemed to them that they also had two legs, but Snowball proved to them that this was not so.

    `A bird's wing, comrades,' he said, `\textsc{is} an organ of propulsion and not of manipulation.
    It should therefore be regarded as a leg.
    The distinguishing mark of Man is the \emph{hand}, the instrument with which he does all his mischief.'

    The birds did not understand Snowball's long words, but they accepted his explanation, and all the humbler animals set to work to learn the new maxim by heart.
    \textsc{four legs good, two legs bad}, was inscribed on the end wall of the barn\dots%
    \mbox{ }\hfill\mbox{(\cite[25]{Orwell:1976aa})}%
    \newline
  \end{scenario}

  \noindent%
  The agents of interest are the birds, and the conclusion is the essential principle of Animalism:
  \propI{Four legs good, two legs bad} has value \valI{True}.

  Snowball provided an argument against an objection from the birds, and the birds concluded \propI{Four legs good, two legs bad} is \valI{True} from Snowball's explanation.

  Though, as \citeauthor{Orwell:1976aa} highlights, the birds did not conclude \propI{Four legs good, two legs bad} from \emph{the content of} Snowball's explanation.
  (Some words were too long.)
  Instead, the birds concluded, at least in part, from (the existence of) \emph{Snowball's explanation}.

  % In parallel to the agent's understanding of arithmetic from~\autoref{illu:gist:calc} the birds of~\autoref{scen:animalism} did not, in part, conclude \propI{Four legs good, two legs bad} from the content of Snowball's explanation.
\end{note}

\begin{note}
  In line with the abstraction given, we may say:
  The birds conclude \propI{Four legs good, two legs bad} has value \valI{True} from the existence of Snowball's explanation.
  And, for each bird, a \ros{} holds between \propI{Four legs good, two legs bad}, \valI{True}, and the existence of Snowball's explanation.
  {
    \color{red}
    And, \wit{0}.
  }
\end{note}

\section*{\qWhy{}, \qHow{} and \issueInclusion{}}
\label{cha:intro:why-how}

\begin{note}
  % Our interest is understanding the way an event in which some agent concludes some proposition \(\phi\) has some value \(v\) from some \pool{} \(\Phi\) happens.
  \phantlabel{how-and-why-first-mention}%
  We distinguish two questions; `\qWhy{}' and `\qHow{}':%
  \footnote{
    \qWhy{} and \qHow{} are questions about the event.
    Natural variation on the questions are about the agent:

    \begin{quote}
      Given \(e\) is an event in which \vAgent{} concludes \(\phi\) has value \(v\):
    \begin{itemize}
    \item
      Which \ros{} between \prop{1}, \val{1}, and \pool{1} explain \emph{why} \vAgent{} concludes \(\phi\) has value \(v\) in \(e\)?
    \end{itemize}
  \end{quote}

  \begin{quote}
    Given \(e\) is an event in which \vAgent{} concludes \(\phi\) has value \(v\):
    \begin{itemize}
    \item
      Which events explain \emph{how} \vAgent{} concluded \(\phi\) has value \(v\) in \(e\)?
    \end{itemize}
  \end{quote}

  With \qWhy{}, thing is \prop{0}-\val{0}-\pool{0}.
  Capturing the way things are from the \agpe{}.
  So, maybe entails variant.
  However, variant suggests important part of explanation is agency.
  Why the agent performed the action in contrast to why an action was performed by the agent.

  This gets difficult.
  For, agency, \agpe{}.
  \agpe{}, then not clearly factive.
  Primary benefit is a clear factive third-person perspective.
  Downside is agency.
  Upside is plausible link to agency.
  Return to this at the end of the document.
  }

  \begin{question}{questionWhy}{\qWhy{}}
    \cenLine{
      \begin{VAREnum}
      \item
        Agent: \vAgent{}
      \item
        \prop{2}: \(\phi\)
      \item
        \val{2}: \(v\)
      \item
        Event: \(e\)
      \item
        \mbox{ }
      \end{VAREnum}
    }
    \medskip

    Given \(e\) is an event in which \vAgent{} concludes \(\phi\) has value \(v\):
    \begin{itemize}
    \item
      Which \ros{} between \prop{1}, \val{1}, and \pool{1} explain \emph{why} \(e\) is an event in which \vAgent{} concludes \(\phi\) has value \(v\)?
    \end{itemize}
    \vspace{-\baselineskip}
  \end{question}

  \begin{question}{questionHow}{\qHow{}}
    \label{q:how}
    \cenLine{
      \begin{VAREnum}
      \item
        Agent: \vAgent{}
      \item
        \prop{2}: \(\phi\)
      \item
        \val{2}: \(v\)
      \item
        Event: \(e\)
      \item
        \mbox{ }
      \end{VAREnum}
    }
    \medskip

    Given \(e\) is an event in which \vAgent{} concludes \(\phi\) has value \(v\):
    \begin{itemize}
    \item
      Which events explain \emph{how} \(e\) is an event in which \vAgent{} concludes \(\phi\) has value \(v\)?
    \end{itemize}
    \vspace{-\baselineskip}
  \end{question}
\end{note}

\begin{note}
  \qWhy{} seeks an understanding of the way an agent concluded \(\phi\) has value \(v\) in terms of \ros{} between \prop{1}, \val{1}, and \pool{1}.

  \qHow{} seeks an understanding of the way an agent concluded \(\phi\) has value \(v\) in terms of what happened.
\end{note}

\begin{note}
  \qWhy{} seeks `explanatory reasons', summarised by \citeauthor{Hieronymi:2011aa} (\citeyear{Hieronymi:2011aa}) as:
  %
  \begin{quote}
    [T]he reasons why things happen, or why things are the way they are.\newline
    \mbox{ }\hfill\mbox{(\citeyear[410]{Hieronymi:2011aa})}
  \end{quote}
  %
  To borrow an an example given by \citeauthor{Hieronymi:2011aa}, the extreme heat or the faulty construction is a reason why the engine failed (\citeyear[409]{Hieronymi:2011aa}).
  Likewise, the engine failing may be a reason why the steamboat is moored, and the steamboat being moored is a reason why the hotel is fully booked, etc.

  However, constraint on reasons.
  \ros{3}.
  Which \prop{0}-\val{0} pairs follow from \pool{1}, from the relevant \agpe{}.
\end{note}

\begin{note}
  In general, `why?' and `how?' are distinct questions.
  To illustrate, consider the following \scen{}:

  \begin{scenario}[England AD 932]
    \label{scen:king}
    \vspace{-\baselineskip}
    \begin{screenplay}
    \item[OLD WOMAN:]
      Well, how did you become King, then?
    \item[ARTHUR:]
      The Lady of the Lake, her arm clad in purest shimmering samite, held Excalibur aloft from the bosom of the waters to signify that by Divine Providence\space\dots\space I, Arthur, was to carry Excalibur\dots\space that is why I am your King.
    \item[DENNIS:]
      Look, strange women lying on their backs in ponds handing over swords\space\dots\space that's no basis for a system of government.
      Supreme executive power derives from a mandate from the masses not from some farcical aquatic ceremony.\newline
      \mbox{ }\hfill\mbox{(\cite[8--9]{Cleese:1974aa})}
    \end{screenplay}
    \vspace{-\baselineskip}
  \end{scenario}

  The old woman asks Arthur \emph{how} the become king.
  Arthur provides an answer in terms of some events which happened, but emphasises that those events are \emph{why} Arthur is king.
  In turn, Dennis accepts the answer provided by Arthur as an account of how Arthur became king but rejects the answer an account of why Arthur is king.
  Instead, an answer is expected in involve the absence of an appropriate system of governance in England.
  So, the answer provided by Arthur is accepted by Dennis as an answer to how, but not as an answer to why.

  The distinction between `why?' and `how?' present in \autoref{scen:king} parallels \qWhy{} and \qHow{} with respect to conclusions.
  In \autoref{illu:gist:calc}, the agent considered performing mental arithmetic, but the consideration of calculating \gistCalcEq{} seems no basis for the conclusion.
  Instead, it is the use of the calculator which is a basis.

  And, when we consider the \ros{} between \propM{\gistCalcEq{}}, \valI{True}, and the calculator, we understand why (and not merely how) the agent concluded \gistCalcEq{}.
\end{note}

\begin{note}
  So, \qWhy{} and \qHow{} are plausibly distinct questions.
  Still, the following constraint on answers to \qWhy{} in terms of answers to \qHow{} seems intuitive:

  \begin{constraint}{consInclusion}{\issueInclusion{}}
    \mbox{ }
    \vspace{-\baselineskip}
    \begin{itemize}
    \item
      \begin{itenum}
      \item[\emph{If}:]
        Some \ros{} between \prop{0}, \val{0}, and \pool{} is, in part, an answer to \qWhy{}.
      \item[\emph{Then}:]
      An event in which the agent concludes the proposition has the value from the \pool{} is, in part, \qHow{}.
    \end{itenum}
  \end{itemize}
  \vspace{-\baselineskip}
  \end{constraint}

  In short, for any \ros{} between a \prop{0}, \val{0}, and \pool{} which grants some understanding of the way an agent concluded \(\phi\) has value \(v\), there is an event such that the agent concludes the proposition has the value from the \pool{}.%
  \footnote{
    What happened does not entail a corresponding \ros{}.
  For example, in \autoref{illu:gist:calc} the agent presses a button marked `\(=\)'.
  However, the agent does not conclude \propM{\gistCalcEq{}} has value \valI{True} \emph{from} pressing the button marked `\(=\)'.
  Hence, no \ros{} seems to hold between \propM{\gistCalcEq{}}, \valI{True}, and the pressing the button marked `\(=\)'.
  }
\end{note}

\begin{note}
  \scen{3}~\ref{illu:gist:calc}~and~\ref{scen:animalism} seem compatible with \issueInclusion{}:%
  \footnote{
    The \illu{1} of \qWhy{} by \scen{3}~\ref{illu:gist:calc}~and~\ref{scen:animalism} is somewhat limited, as no \scen{0} involves an account of what motivates the respective agents.
    \qWhy{} is not designed to rule out what motivates an agent.
    For example, one may add to \autoref{illu:gist:calc} that the agent wants to calculate \gistCalcLHS{}.
    Functions as a premise, given the understanding of \pool{1}.
    More detail follows in~\autoref{cha:clar}.
  }

  \begin{itemize}[noitemsep]
  \item
    The only \ros{} which seems to answer \qWhy{} with respect to~\autoref{illu:gist:calc} is the \ros{} between \propM{\gistCalcEq{}}, \valI{True}, and something about the calculator.
    And, the agent concludes \propM{\gistCalcEq{}} has value \valI{True} from the calculator.
    Hence, there is a corresponding answer to \qHow{}.

    A \ros{} may hold between \propM{\gistCalcEq{}}, \valI{True} and whatever \pool{} would be associated with the agent calculating \(\gistCalcEq{}\) from their understanding of Arithmetic.
    However, there is no event in which the agent concludes \propM{\gistCalcEq{}}, has value \valI{True} by performing arithmetic.
    So, there is no answer to \qHow{} with respect to the agent's understanding of arithmetic, and given \issueInclusion{}, it is not possible for a \ros{} to answer \qWhy{}.
  \item
    \begin{itemize}
    \item
      The \ros{} between \propI{Four legs good, two legs bad}, \valI{True} and something about the existence of Snowball's explanation.
    \item
      The birds conclude \propI{Four legs good, two legs bad} has value \valI{True} from the existence of Snowball's explanation.
    \end{itemize}

    The bird's conclusion \propI{Four legs good, two legs bad} has value \valI{True} from the existence of Snowball's explanation answers \qHow{}, and the \ros{} between the \prop{0}, \val{0}, and \pool{} answers \qWhy{}, in line with \issueInclusion{}.

    And, while a \ros{} may hold between \propI{Four legs good, two legs bad} \valI{True} and the \emph{content} of Snowball's explanation, the birds failed to understand the content.
    Hence, with no answer to \qHow{}, it is not possible for this \ros{} to answer \qWhy{}.
  \end{itemize}
  %
  These \ros{} seems to do work in guiding our understanding of the an event.
  To observe that a \ros{} held indicates something in addition to the observation of what happened, but for a \ros{} to answer \qWhy{} it seems we need a `\wit{}' for the respective.
  Hence, \issueInclusion{} requires that every \ros{} which answers \qWhy{} has some \wit{}.
\end{note}

\begin{note}
  \issueInclusion{} seems plausible, and seems to do work.%
  \footnote{
    Note, \qHow{} does not explicitly require the relevant event to be the event in which the agent concludes \(\phi\) has value \(v\) from the \pool{}.
    Hence, a previous event in which the agent concluded \propM{\gistCalcEq{}} has value \valI{True} may answer \qHow{}.
    Still, given \(\gistCalcEq{}\) in~\autoref{illu:gist:calc} functions as some arbitrary multiplication that the agent may calculation, we may assume the agent has never calculated \(\gistCalcEq{}\).
  }\(^{,}\)%
  \footnote{
    Alternatively, \issueInclusion{} may be thought to narrow the range on answers to \qWhy{}.
    That is to say, \issueInclusion{} functions to disambiguate the sense of `why' used in the statement of \qWhy{}.
    If so, then \issueInclusion{} is not a constraint, it is simply part of the way \qWhy{} is understood.
    However, I do not think this is the case.
    I think it is plausible that the sense of `why' present in \qWhy{} may be understood without reference to \issueInclusion{}, and hence that \issueInclusion{} amounts to a substantive constraint.
    The document will assume this is the case, but only the framing depends on this.
  }

  Still, we have only considered two \scen{1} and \issueInclusion{} is a general constraint.
  Hence, if there is some doubt regarding \issueInclusion{} then further argument is required.
  It is not obvious that there are \scen{1} in which \issueInclusion{} fails to hold.
  However, lack of immediate counterexamples is no argument.

  Though I would like to consider an argument for \issueInclusion{}, I am not aware of any.%
  \footnote{
    The particular construct of \qWhy{}, \qHow{}, and \issueInclusion{} is somewhat idiosyncratic, and so it is no surprise there are no direct argument for \issueInclusion{}.
    However, I take the idea captured by \issueInclusion{} to be intuitive, and I am not aware of any argument for this idea.
  }
  Perhaps \issueInclusion{} is sufficiently intuitive that it is taken for granted.
  Or, perhaps the failure of \issueInclusion{} is recognised and I have yet to stumble upon the acknowledgement of its failure.

  {
    \color{red}
    Still, constraint on failure.
    For, ask \qWhy{} with respect to an event where agent concludes \(\phi\) has value \(v\) from \(\Phi\).
    So, there is always an answer to \qHow{} with respect to \ros{} between \(\phi\), \(v\), and \(\Phi\).

    Hence, for \issueInclusion{} to fail, it must be the case that some distinct \ros{} answers \qWhy{}, and no `\wit{}'.

    At issue, then, is whether this is ever the case.
    Seems clear with initial \scen{1}.
    Is this really the case in general?
    This is what interest with \issueInclusion{} hangs on.

    Go a little further here.
    It needs to be the case that \ros{} `directly' answers.
  }

  In any case, no argument for \issueInclusion{} will be given.
  The goal of this document is to provide a recipe for generating counterexamples to \issueInclusion{}.

  By `recipe', I mean an description of some features which hold of \scen{1} and an account of the way in which these combine which entails violations of \issueInclusion{}.
  With the recipe in hand, we will construct a few counter samples, but our interest is with the way in which \issueInclusion{} fails, rather than the failure of \issueInclusion{}.

  Before closing this introduction, say a little more regarding the recipe for counterexamples.
  First, we consider motivation for \issueInclusion{} via the literature on reasons.
  I am not aware of any arguments for \issueInclusion{}, but I think there is strong motivation for \issueInclusion{}.%
  \footnote{
    \color{red}
    And, refinement to questions, provide further motivation.
  }
\end{note}

\section*{\issueInclusion{} and rationalisations}
\label{sec:reasons}

\begin{note}
  \qWhy{} and \qHow{} are questions about the way an event in which an agent concludes some proposition has some value happened.
\end{note}

\begin{note}
  Motivation by two distinct ideas about explanation in terms of an agent's reasons.
  First, \cExpl{0}.
  Second, \fExpl{0}.
\end{note}

\paragraph*{Constraints on rationalisations}

\begin{note}
  \citeauthor{Davidson:1963aa} opens \citetitle{Davidson:1963aa} with the following question:
  %
  \begin{quote}
    What is the relation between a reason and an action when the reason explains the action by giving the agent's reason for doing what he did?
    We may call such explanations \emph{rationalizations}, and say that the reason \emph{rationalizes} the action.%
    \mbox{}\hfill\mbox{(\citeyear[685]{Davidson:1963aa})}
  \end{quote}
  %
  \citeauthor{Davidson:1963aa}'s interest is with explanatory reasons \emph{such that} the explanatory reason involves the agent's reason.%
  \footnote{
    These may also be understood, in line with \citeauthor{Smith:1994wo} (\citeyear{Smith:1994wo}), as `motivating reasons':
    \begin{quote}
      The distinctive feature of a motivating reason to \(\phi\) is that, in virtue of having such a reason, an agent is in a state that is \emph{explanatory} of her \(\phi\)-ing, at least other things being equal --- other things must be equal because an agent may have a motivating reason to \(\phi\) without that reason's being overriding.%
      \mbox{}\hfill\mbox{(\citeyear[96]{Smith:1994wo})}
    \end{quote}
  }
  The main argument of \citetitle{Davidson:1963aa} is:
  %
  \begin{quote}
    [R]ationalization is a species of ordinary causal explanation.%
    \mbox{ }\hfill\mbox{(\citeyear[685]{Davidson:1963aa})}
  \end{quote}
  %
  So, a reason explains the action by giving the agent's reason for doing what he did when the agent's reason causes the action.

  \citeauthor{Davidson:1963aa} constrains a `why' question by a `how' question, given a background constraint.
  Why question is `why did the agent perform the action?'.
  Background constraint is that the why question is answered by rationalisations.
  How question is `how did agent cause the action?'.
  How constrains why, as any rationalisation is a cause.
\end{note}

\begin{note}
  For \citeauthor{Davidson:1963aa}, cause.
  However, the causation is not required.

  Consider \citeauthor{Hieronymi:2011aa}'s (\citeyear{Hieronymi:2011aa}) account of rationalisation:%
  \footnote{
    Also, \citeauthor{Harman:1973ww}:
    %
    \begin{quote}
    Reasons may or may not be causes; but explanation by reasons is not causal or deterministic explanation.
    It describes the sequence of considerations that led to belief in a conclusion without supposing that the sequence was determined.%
    \mbox{ }\hfill\mbox{(\citeyear[52]{Harman:1973ww})}
  \end{quote}
  }

  \begin{quote}
    [W]henever an agent acts for reasons, the agent, in some sense, takes certain considerations to settle the question of whether so to act, therein intends so to act, and executes that intention in action.

    If this much is uncontroversial (and, under some interpretation, I believe it must be), we can use it as a form for filling out.
    I propose, then, that we explain an event that is an action done for reasons by appealing to the fact that the agent took certain considerations to settle the question of whether to act in some way, therein intended so to act, and successfully executed that intention in action.
    I suggest that \emph{this} complex fact [\dots] is the reason that rationalizes the action---that explains the action by giving the agent's reason for acting.%
    \mbox{ }\hfill\mbox{(\citeyear[421]{Hieronymi:2011aa})}
  \end{quote}

  \citeauthor{Hieronymi:2011aa}'s (complex) fact is about how the agent acted.
  Agent settled question of whether to act in some way.
  That these considerations capture how agent settled, this captures why.
  So, in this case why coincides with how.
  And, \citetitle{Hieronymi:2011aa} states this fact is sufficient.
\end{note}

\paragraph*{Rationalisations and \ros{1}}

\begin{note}
  \citeauthor{Davidson:1963aa}'s claim that rationalisation is a species of ordinary causal explanation and \citeauthor{Hieronymi:2011aa}'s claim that rationalisations are considerations taken to settle a question of whether to act in some way \emph{suggest} constraints similar to \issueInclusion{}.
\end{note}

\begin{note}
  Still, to argue \issueInclusion{} follows from either \citeauthor{Davidson:1963aa}'s or \citeauthor{Hieronymi:2011aa}'s account of rationalisations would require:

  \begin{itemize}
  \item
    \emph{If} a \ros{} answers \qWhy{} \emph{then} the \ros{} is a rationalisation.
  \end{itemize}

  Intuitively, certain \ros{} correspond to rationalisations.
  For example, we may casually say with relative ease:%
  \footnote{
    This intuition ignores details about what \citeauthor{Davidson:1963aa} and \citeauthor{Hieronymi:2011aa} think reasons are.

    For \citeauthor{Davidson:1963aa} specifying:
    \begin{enumerate*}[label=(\alph*), ref=(\alph*)]
    \item
      Some sort of pro attitude (a want, obligation, etc.) toward actions of a certain kind, and
    \item
      a belief (or perception, notice, etc.) that his action is of that kind.
    \end{enumerate*}
    Is sufficient to characterise the agent's reason for doing something.
    (\citeyear[Cf.][685,686]{Davidson:1963aa})

    Applied to conclusions, strange results.
    Pro attitude toward conclusions, and a belief that performing the action is a conclusion.
  }

  \begin{itemize}[noitemsep]
  \item
    The reason for which the agent concluded \propM{\gistCalcEq{}} is \valI{True} was the calculator.
  \item
    The reason for which the birds concluded \propI{Four legs good, two legs bad} is \valI{True} was (the existence of) Snowball's explanation.
  \end{itemize}

  Still, interest is with other \ros{1} answer \qWhy{}.
  And, it seems any other \ros{} fails to be rationalisation.

  For, with rationalisation we've fixed the relevant action.
  It's the conclusion \(\phi\) has value \(v\) from \(\Phi\).
  \ros{} is between \(\psi\), \(v'\) and \(\Psi\).

  If \ros{} gets action, then this does not answer \qWhy{}.

  Here, need an additional premise.
  An agent's reason is all that matters from the agent's perspective.
  But, I don't see a clear argument for this from \citeauthor{Davidson:1963aa}.
  For, the point is that we're looking at a relation between reason and action.
  Any distinct \ros{} may explain the action, but is not going to be relation of the right kind.

  For example, consider calculator.
  Suppose, instead, arithmetic.
  Now, get the agent's understanding of arithmetic.
  Somehow this is a cause.
  However this works out, I think this is fine.
  May hold that relation holds between various other equations.
  Granting this, there is not immediate entailment to relations.

  Here, borrow the Tolliver example, maybe.

  At issue is whether these \ros{} are relevant\dots
  Clear that these are not causally involved.
\end{note}

\begin{note}
  Motivation.
  Difficulty here is that both \citeauthor{Davidson:1963aa} and \citeauthor{Hieronymi:2011aa} are concerned with an agent's reason(s).

  It is not clear that \ros{} which answers \qWhy{} captures agent's reasons.

  For example, steamboat being moored, hotel.
  However, not in terms of hotel's reasons.
  Simply too many guests.

  In general, lots of this explain why.

  I do not think \issueInclusion{} is too distinct.
  \ros{} is from the \agpe{}.
  Hence, answers to \qWhy{} are close.
  But this is still distinct from a reason.
\end{note}

\paragraph{Answers to \qWhy{} contrasted to rationalisations}

\begin{note}
  So, what exactly is \qWhy{} asking?

  Well, one perspective, whether it matters what else the premises do from the \agpe{}.
  Special case where \(\Psi = \Phi\).

  Here, then, agent concludes \(\phi\) has value \(v\) from \(\Phi\).
  A \ros{} answers \qWhy{}.
  Does it matter that a \ros{} holds between \(\psi\), \(v'\) and \(\Phi\)?

  Variant calculator case above.

  More general case is distinct.
  These are original calculator case.
  \ros{} seems irrelevant.
\end{note}

\begin{note}
  Subtle issue.
  Agency.
\end{note}

\section*{Questioning \issueInclusion{}}

\begin{note}
  Our interest is understanding the way an event in which some agent \vAgent{} concludes some proposition \(\phi\) has some value \(v\) happened.

  We introduced two questions (\qWhy{}, \qHow{}) and a constraint (\issueInclusion{}) on answers to \qWhy{} by answers to \qHow{}.
  And, we motivated the constraint in part by a pair of \scen{1} (\scen{3}~\ref{illu:gist:calc}~and~\ref{scen:animalism}) and then more generally in terms of reasons.
  %(\cite{Davidson:1963aa},\cite{Broome:2019aa},\cite{Hieronymi:2011aa})

  I think \issueInclusion{}, while intuitive, fails to hold in general.
\end{note}

\begin{note}
  I think that answers to \qWhy{} reduce to psychological facts of an agent, specifically psychological facts which hold of the agent when they conclude.%
  \footnote{
    This is incompatible with views of reasons explanation advanced by~(\cite{Dancy:2000aa}) and~(\cite{Alvarez:2013aa}), in which the state of affairs may be an agent's reason (and not just the evaluation of some state of affairs).
    Though, the argument will not depend on assuming that the relations reduce to psychological facts.

    See~(\cite[413--418]{Hieronymi:2011aa}),~(\cite[3--5]{DOro:2013vh}), and~(\cite[\S2]{Alvarez:2017aa}) for more.
  }
  However, I do not think that a relation between the \prop{0}, \val{0}, and \pool{} exhaust the relevant psychological facts.
  In particular, I think that in various cases additional relations between distinct \prop{1}, \val{1} and \pool{1} answer \qWhy{} without there being an event in which the agent concluded the propositions have values from \pool{1}.
\end{note}

\begin{note}
  As seen above with respect to \autoref{scen:animalism}, it follows from \issueInclusion{} that the content of Snowball's explanation is irrelevant.
  For, the birds do not understand Snowball's explanation, hence there is no event in which the birds reason from the content to \propI{Four legs good, two legs bad} has value \valI{True}.
  And, I think this is correct.

  However, parallel reasoning entails the agent's understanding of arithmetic is irrelevant with respect to \autoref{illu:gist:calc}.
  And, I do not think this entailment holds.
  I think it may be the case that a relation between \propM{\gistCalcEq{}}, \valI{True} and the agent's understanding of arithmetic \emph{may} answer \qWhy{}.
  Whether this is the case will depend on whether some additional details hold of~\autoref{illu:gist:calc}.
  Still, as the details matter, the entailment, and hence \issueInclusion{}, is not right.
\end{note}

\begin{note}
  In other words, I think there are counterexamples to \issueInclusion{}.
  The primary goal of this document is to provide a recipe for generating counterexample to \issueInclusion{}.
\end{note}

\paragraph*{Structure}

\begin{note}
  The document has three parts:

  \begin{TOCEnum}
  \item
    \autoref{part:prep}: \nameref{part:prep}.

    We begin by clarifying our understanding of conclusions, \qWhy{}, \qHow{}, and \issueInclusion{}.
    In particular, we provide variations to \qWhy{}, \qHow{}, and \issueInclusion{} in order to precisely capture what the counterexamples we generate our counterexamples to.
  \item
    \autoref{part:ing}: \nameref{part:ing}

    We detail three ideas which will be used to generate counterexamples.
    Each idea captures some phenomenon.
    The counterexamples occur when each idea applies to a \scen{0} in which an agent concludes some proposition has some value from some \pool{}.

    The key idea is that of a \fc{}.
    However, the ideas of \tC{}, and \requ{} will tie \fc{1} to instances in which an agent concludes.
  \item
    \autoref{part:dir}: \nameref{part:dir}

    With the preparation and ingredients in hand, we show how to combine the ingredients to generate counterexamples to \issueInclusion{}.

    We also provide a few sample \scen{1} where \issueInclusion{} fails, and consider any leftover issues from the recipe.
    Here we return to \autoref{illu:gist:calc}.
  \end{TOCEnum}

  This covers how things will happen.
  I do not have a brief account of what will happen.
  The details are too important.
  The recipe is not based on an intuitive understanding of any particular \scen{}.
  Instead, the recipe is based on the way a number of ideas come together when we understand the way some agent concludes some proposition \(\phi\) has some value \(v\) happened.

  The beginning of each part has additional details on the way in which the part is structured.
  Brief account of the chapters, and occasionally supplementary details.
\end{note}


% \begin{note}
%   First, expand on intuition.
%   Then, introduce type of \scen{0} of interest.
% \end{note}

% \begin{note}
%   Follow part, foundations.
%   Following, turn to details.

% {\color{red} Place somewhere?}.%
%   \footnote{
%     Roughly, if it were the agent failed to conclude \propM{\gistCalcEq{}} in \autoref{illu:gist:calc}, then there would be conflict between the agent's understanding of arithmetic and the testimony of the calculator.
%     Expressed differently, there would be conflict between the agent's failure to conclude \propM{\gistCalcEq{}} by their understanding of arithmetic, and a premises involved in concluding \propM{\gistCalcEq{}} via the calculator.
%     I.e. supposing the agent concludes \(\gistCalcLHS{} \ne \gistCalcRHS{}\), then for the agent the calculator is not a source of testimony.
%     In the \scen{1} of interest, this hypothetical --- or in some cases possible --- conflict will strictly be between the agent's reasoning from \pool{1} to conclusions.
%   }
% \end{note}

% \begin{note}
%   To illustrate, consider \citeauthor{Broome:2013aa}'s (\citeyear{Broome:2013aa}) account of a `motivating reason'%
%   \footnote{
%     \citeauthor{Broome:2013aa} contrasts `motivating reasons' to `normative reasons'.
%     \begin{quote}
%       Whereas motivating reasons explain or help to explain why a person does something, normative reasons explain or help to explain why a person ought to do something, or to believe something, or to hope for something, or to like something, or in general to F, where ‘F' stands for a verb phrase.%
%     \mbox{}\hfill\mbox{(\citeyear[47]{Broome:2013aa})}
%     \end{quote}
%   }
%   \begin{quote}
%     Sometimes the explanation of why a person does something has a particular character:
%     roughly, it involves the person's rationality in a distinctive way that I shall not try to describe.
%     Then we say the person does what she does for a reason.
%     We might say ‘The reason for which Hannibal used elephants was to terrorize the Romans'.
%     The reason for which a person does something is called a ‘motivating reason'.
%     In general, a motivating reason is whatever explains or helps to explain what a person does in the distinctive way that involves her rationality.
%     \mbox{}\hfill\mbox{(\citeyear[46--47]{Broome:2013aa})}
%   \end{quote}
% \end{note}

% \begin{note}
%   \color{red}

%   Further,

%   For \citeauthor{Davidson:1963aa}, primary reason.

%   \begin{quote}
%     \emph{R} is a primary reason why an agent performed the action \emph{A} under the description \emph{d} only if \emph{R} consists of a pro attitude of the agent toward actions with a certain property, and a belief of the agent that \emph{A}, under the description \emph{d}, has that property.\newline
%     \mbox{ }\hfill\mbox{(\citeyear[687]{Davidson:1963aa})}
%   \end{quote}

%   We have distinguished \qWhy{} from pro-attitudes.
%   However, fill in whatever motivation.
%   What matters is the belief.
%   This is the relevant proposition-value pair.

%   If \citeauthor{Davidson:1963aa}, then granting restriction, seems we don't need to look beyond the proposition-value pair.
% \end{note}

% \begin{note}
%   So, answer to \qWhy{} is constrained by answer to \qHow{} by getting to a reason.

%   For, causal relation.
%   Indeed, from `explanatory', these things are identical.
%   However, from `motivating' still distinction.
%   Agent's reason is causal, but the content is not necessarily causal.%
%   \footnote{
%     On my understanding of \citeauthor{Davidson:1963aa}, there's a tight link between then content of some state and the causal relations that arise from the state.

%     So, go from content to state, and then proceed from here.

%     This, I think, is correct.
%     And, the problem of deviant causal chains highlights this.
%     For, \citeauthor{Davidson:1963aa} recognises there's a problem with the link between content and the causal relations which hold between the states.

%     \begin{quote}
%       Beliefs and desires that would rationalize an action if they caused it in the right way—through a course of practical reasoning, as we might try saying---may cause it in other ways.%
%       \mbox{ }\hfill\mbox{(\citeyear[79]{Davidson:1973vd})}
%     \end{quote}
%   }

%   Here, we get a causal trace.
%   No need to look for any relation of support other than premises of reasoning.
% \end{note}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% TeX-engine: luatex
%%% End:
