%%% Local Variables:
%%% TeX-master: "master"
%%% End:


\chapter{Inertia}
\label{cha:inertia}

\section{Inertia}
\label{sec:inertia}

Argument against~\ref{denied-claim} is that it conflicts with \nI{-} in scenarios highlighted by~\ref{prem:ab}.

\begin{enumerate}[label=\nI{}, ref=\nI{}]
\item An agent is not able to obtain support for some proposition \(\psi\) on the basis of information that some the support the agent has for \(\phi\) is misleading or mistaken if \(\psi\) is not the case.
\end{enumerate}

There is lack of an explanatory connexion between support for \(\phi\) and support for \(\psi\).

\section{nI}
\label{sec:ni}

Case in which the support the agent has does not trace from access, or the agent obtains support on the basis that the support the agent does have is misleading.

Note, talk about `obtaining' support.
So, we have a setup where:

\begin{enumerate}
\item Agent has support.
\item Say, for some proposition \(\psi\).
\item The support for \(\psi\) is not, given the agent's doxastic state, also support for \(\psi\).
\item Agent receives information to the extent that the agent's support for \(\psi\) is misleading if \(\psi\) is not the case.
\item Agent holds that the support for \(\psi\) supports \(\phi\) being the case, because else their support is misleading.
\end{enumerate}

Here, the support doesn't do anything for \(\phi\).

This principle is less obvious, but I think fine.

Consider a few good cases and some bad cases.

Good case, deduction.
So, if support for \(\phi\) then support for \(\phi \lor \psi\), because, intuitively, support for \(\phi\) is also support for \(\phi \lor \psi\).
It is puzzling to motivate on the basis that the agent obtains support for \(\phi \lor \psi\) because if \(\phi \lor \psi\) is not the case then their support for \(\phi\) would be misleading.
Of course, this is the case.
However, that is not why the agent has support for \(\phi \lor \psi\).


Suppose quiz.
Interested in answer to a certain question, mix with questions I know the answer to.
Contestant gets the answers correct.
Support that the contestant is good with respect to the domain.
Have support for the question I'm interested in.

Problem if that I hold the answer to the question is such and such because otherwise support for contestant understanding the domain would be misleading.

These are two cases where things look straightforward, and the agent has support by some other route.

If X then support is misleading
Therefore, not-X.

???

If that's not a boat then the radar is faulty.

Have support that the radar is working well.
So, it is a boat.

??? Though in this case it looks as though there's a way to go from functioning radar to boat.

Slight variant.
Checked the radar, support that it's functioning correctly.
Not looking at the screen.

If there's a blip on the radar, then it's not functioning correctly.
Support that it's functioning correct.
So, there's no blip on the radar.



Parked car outside.
If car is stolen then support that this is a safe neighbourhood would be misleading.
Support for car is not stolen.


If the seeds haven't started sprouting yet, then you've been misled about the planting conditions.
Have not been misled about the planting conditions.
So, the seeds have started sprouting.

\begin{note}[Denying~\nI{}]
  Worry about coherence.

  If the agent does not obtain support, then in a situation where:
  \begin{itemize}
  \item Support for \(\phi\)
  \item Information that if \(\phi\) is the case then \(\psi\) is the case.
  \item No support for \(\psi\).
  \end{itemize}

  So, agent is committed in some sense to \(\psi\) being the case, given the support they have for \(\phi\) and the information.

  I have support for \(\phi\) and information that \(\phi \rightarrow \psi\), but no support for \(\psi\).

  Looks like a failure of closure.

  However, closure is typically formulated with entailment.
  In each of these cases, there's no entailment.
  Distinction between the support for \(\phi\) and \(\phi\).

  Would need something to the effect of positive attitude only if support.

  However,~\nI{} only denies support.
  It does not deny that the agent is require to have some positive attitude toward the proposition.
\end{note}

\begin{note}[Undercutting defeaters]
   Undercutting defeaters are of interest with respect to~\nI{} is that (merely) suggesting that the agent's support for \(\phi\) is no good can amount to highlighting that the agent's support for \(\phi\) may be misleading.

  Consider the following illustration provided by \citeauthor{Pollock:1987un}\nolinebreak
  \footnote{
    \citeauthor{Pollock:1987un} defines an undercutting defeater as follows:
    \begin{quote}
      R is an \emph{undercutting defeater} for P as a prima facie reason for S to believe Q if and only if
      \begin{enumerate}[label=(UD\arabic*), ref=(UD\arabic*)]
      \item P is a reason for S to believe Q and R is logically consistent with P but (P and R) is not a reason for S to believe Q, and
      \item R is a reason for denying that P wouldn't be true unless Q were true.\nolinebreak
        \mbox{}\hfill\mbox{(\citeyear[485]{Pollock:1987un})}
      \end{enumerate}
    \end{quote}
    Intuitively, an undercutting defeater for P as a reason for Q because it the defeater denies that Q must be true in order for P to be true.
  }\nolinebreak
  :
  \begin{quote}
    [Undercutting defeaters] attack the connection between the reason and the conclusion rather than attacking the conclusion itself.
    For instance, ``X looks red to me'' is a prima facie reason for me to believe that X is red.
    Suppose I discover that X is illuminated by red lights and illumination by red lights often makes things look red when they are not.
    This is a defeater, but it is not a reason for denying that X is red (red things look red in red light too).
    Instead, this is a reason for denying that X wouldn't look red to me unless it were red.\nolinebreak
    \mbox{}\hfill\mbox{(\citeyear[485]{Pollock:1987un})}
  \end{quote}
  Completing \citeauthor{Pollock:1987un}'s example, it seems that if agent's support for holding that X is red is that `X wouldn't look red to me unless it were red', then the support for X being red provided by appearance is retracted after discovering that X is illuminated by red lights (though it remains possible that X is red).

  Assumption --- in violation of \nI{} --- an agent always has the option to obtain support for \(\psi\) on the basis of information that the support the agent has for \(\phi\) is misleading if \(\psi\) is not the case.

  Given that it remains possible that X is red, it follows that the appearance of X being red would be misleading if X is not red.
  For, it continues to be the case that X appears to be red, even after the discovery is made by the agent.
  Therefore, by the assumption violating~\nI{}, support for X being red persists in the presence of the undercutting defeater.
  Still, intuitively it seems that the undercutting defeater of a red light blocks the agent obtaining support for X being red on the basis of their visual perception.

  This is partial motivation for~\nI{}, as substituted `does not have the option' from the text of~\nI{} with `always has the option' when making the problematic assumption.
  Further argument is (hopefully) to follow.

\end{note}


\section{Motivation for \nI{}}
\label{sec:motivation-ni}

\begin{itemize}
\item Because, the support the agent has is independent of \(A(\psi)\)/\(\psi\).
\end{itemize}

\begin{itemize}
\item An agent is not able to obtain support for some proposition \(\psi\) on the basis of information that the support the agent has for \(\phi\) is misleading if \(\psi\) is not the case.
\end{itemize}

\begin{itemize}
\item The relevant information must also provide some support for \(\phi\).
\item One way of getting to this is by ordering support.
\item If the constraint is established prior to obtaining support, then this may limit support.
\end{itemize}

\begin{itemize}
\item This is also related to Harman.
\item For, the principle there is that one is not in a position to hold that support is going to be misleading.
\item Support for \(\phi\) does not show that future support for \(\psi\) is misleading when \(\psi \vdash \lnot\phi\).
\item Support for \(\phi\) does not show that\dots
\end{itemize}

\begin{itemize}
\item Important to note is that this does not deny closure.
\item First, doxastic.
\item Second, no requirement that the required information is a (known, logical) entailment.
\end{itemize}

Why does witnessing work?

\begin{itemize}
\item Because the agent is not basing things on the support they have.
\item The things about misleading support is that it doesn't say there's anything problematic about the information received.
\end{itemize}

\subsection{Motivation, taken from overview}
\label{sec:motiv-taken-from}

\begin{note}[Some motivation for \nI{}, 1]
  Some motivation for~\nI{} may be found by constructing instances of reasoning which violate~\nI{}.
  A couple of cases may help.

  For example, suppose our agent is out shopping for a gift for Sam with a friend and has not yet considered the items before them.
  The friend remarks that `Someone would only buy \emph{that} (some particular item) as a gift for Sam if they didn't know Sam very well.'
  The agent has support that they do know Sam very well.
  However, it does not seem permissible for the agent to obtains support that they wouldn't buy that as a gift, one the basis of the support they have their familiarity with Sam and the statement made by their friend.
  For, it is possible that they agent would have settled on the particular item if they had given it some consideration --- the support the agent has for their familiarity with Sam \emph{may} be misleading.

  Or, suppose the agent has parked their car on the street outside of Sam's place and reasons that:
  If my car is stolen then my support that this is a safe neighbourhood would be misleading.
  Therefore, as my support that this is a safe neighbourhood is not misleading, my car has not been stolen.
  The agent's reasoning seems confused.
  Perhaps the agent has the option of extending the support the have that the neighbourhood is safe to obtain support that they car remains parked outside, but that support the agent has would be misleading if their car has been stolen does not seem appropriate.
\end{note}

\begin{note}[Some motivation for \nI{}, 2]
  For further motivation, consider undercutting defeaters.

  We take the following sketch from \textcite{Worsnip:2018aa}:
  \begin{quote}
    Undercutting defeaters, which are easiest to think of in the context of the attitude of belief, are supposed to be considerations that undermine the justification of a belief in a proposition p not necessarily by providing (sufficient) positive evidence to think that p is false, but rather merely by suggesting (perhaps misleadingly) that one’s reasons for believing p are no good, in a way that neutralizes or mitigates their justificatory or evidential force.\nolinebreak
    \mbox{}\hfill\mbox{(\citeyear[29]{Worsnip:2018aa})}
  \end{quote}
\end{note}


\subsection{Misleading support}
\label{sec:misleading-support}

\begin{note}[Misleading, again]
  By the support that the agent has for some proposition being \emph{misleading} we mean that the support the agent has for \(\phi\) provides good reason for thinking that \(\phi\) is the case, even if \(\phi\) is not the case.\nolinebreak
  \footnote{
    Different from `mistaken', where \(\phi\) turns out to be true, but the support itself provides the wrong understanding of why \(\phi\) is true.

    Suspect that a variant of \nI{} holds for `mistaken'.
    However, `misleading' is sufficient.
  }
  For example, see someone entering a car by unlocking the drivers door with a coat-hanger.
  I take this to be good support for the proposition that the person is not the owner of the car.
  As it turns out, the person had accidentally locked their only set of keys in the car, and did not want to call a locksmith.
  The support for the proposition that the person is not the owner of the car was misleading, not only because the person owns the car, but because it continues to provide support even with the knowledge that the person is the owner.
  It would be clear to the owner why they would be stopped by a police officer, and why the owner is required to provide documentation to show that they are the owner (i.e.\ support to the contrary proposition).

  In this sense, the support is misleading.
  The support is `good', even though the proposition is false.

  `Misleading' support is distinct from an error made by the agent, such as taking the use of the coat-hanger to be support for the proposition that the someone had stolen the person's coat and locked the coat inside the car.\nolinebreak
  \footnote{
    No doubt there is additional background that could be added\dots
  }
  And, misleading support is distinct from `mistaken' in which an agent obtain support for some proposition which is true but where some part of the support is false.

  For example, person using the coat-hanger looks like the owner, but the car had recently been sold.
  In contrast to error, there seems no problem with the agent holding a positive attitude toward the proposition given mistaken and misleading support, the relevant difference is whether the proposition does hold.

  Intuitively, then, the trouble with misleading support is that it suggests, but does not ensure that the proposition holds.

  Misleading support, then, is not problematic.
\end{note}


\begin{note}[Combination]
  \nI{} and \nIm{} combine:

  \begin{proposition}[\nIp{}]
    An agent does not have the option of claiming support for some proposition \(\xi\) from observing that \(\psi\) entails \(\xi\) and information that the support the agent claims for \(\phi\) is misleading if \(\psi\) is not the case.
  \end{proposition}

  With respect to scenarios of interest.
    \(\phi = \) general ability.
    \(\psi = \) specific ability.
    \(\xi = \) strategy exists.

    So, application of \nI{} (and \nIm{}) is that the agent does not have option of obtaining support for specific ability from information that the support they have for general ability is misleading if the agent does not have the specific ability.

    Stated independently, \nIp{} is a weaker proposition than \nI{} and \nIm{}.\nolinebreak
    \footnote{
      as \nIp{} makes no statement about whether the agent may claim support for \(\psi\) (though \nI{} does).

      From \nI{}, the agent does not get support for \(\psi\), and by \nIm{} agent requires support for \(\psi\) to get support for \(\xi\), and hence does not obtain support for \(\xi\).

    Still, as it is the result of interest, some room to pursue the present line of argument if either \nI{} or \nIm{} is rejected.
    }
    Suspect that any argument from \nIp{} follows from motivation for \nI{} and \nIm{}.

  If \nI{} (and \nIm{}) hold, then trouble if support for specific ability comes from general being misleading if general is not the case.
\end{note}


\begin{note}[Relation to transmission]
  Here, difference to failure of transmission.
  In those cases, agent knows, hence \(\phi\) not being true is not much of an issue.
  In turn, \(\psi\).

  For us, \(\phi\) may be false.

  Whether there is a proper distinction, though, isn't quite so easy.
  Dogmatism gets thrown into the mix.
\end{note}


\begin{note}[Things not claimed]
  Don't appeal to possibility that support for \(\psi\) will be stronger for \(\psi\) having different value.
  Support isn't sufficiently strong.\nolinebreak
  \footnote{
    This requirement distinguishes from transmission failure.
  }
  Given that \(\psi\) is a dependency, \(\psi\) may be why strength of support is limited.
  And, from the above, lack of use of support for \(\psi\), hence agent is not in a position to go either way.
\end{note}


\section{Relatated literature}
\label{sec:relatated-literature}

\begin{note}[Related to\dots]
  Focus is on support.
  A pair of related things to illustrate interesting parts of \nI{}.
\end{note}

\begin{note}[No feedback/Bootstrapping]
  No feedback by \citeauthor{Weisberg:2010to}.

  \begin{quote}
    \textbf{No Feedback} If
    \begin{enumerate*}[label=(\roman*), ref=(\roman*)]
    \item\label{WB:NF:1} \(L_{1}-L_{n}\) are inferred from \(P_{1}-P_{m}\), and
    \item\label{WB:NF:2} \(C\) is inferred from \(L_{1}-L_{n}\) (and possibly some of \(P_{1}-P_{m}\) by an argument whose justificatory power depends on making \(C\) at least \(x\) probable, and
    \item\label{WB:NF:3} \(P_{1}-P_{m}\) do not make \(C\) at least \(x\) probable without the help of \(L_{1}-L_{n}\), then the argument for \(C\) is defeated.\nolinebreak
      \mbox{}\hfill\mbox{(\citeyear[533--534]{Weisberg:2010to})}
    \end{enumerate*}
  \end{quote}

  Some similarities.
  Stated in probabilistic terms, so no easy application.
  And, addition of \(C\) condition.
  Expand to add this.

  Somewhat different.
  For, inclusion of support, which (at least without further work) is qualitative rather than quantitative.
  So, it seems \ref{WB:NF:3} isn't going to hold.

  Still, reform No Feedback a little.
  Consider \ref{WB:NF:3} from the agent's point of view, independent of whether \(P_{1}-P_{m}\) not make \(C\) at least \(x\) probable.
  Plausibly problematic if agent doesn't have response to~\ref{WB:NF:3}.
  And, this would be issue of not showing how \(P_{1}-P_{m}\) relate to \(L_{1}-L_{n}\).
\end{note}

\begin{note}[Bootstrapping and reliabilism]
  \nI{} is distinct from No Feedback, but shares some similarities.
  Attempted to keep to weak principles regarding support --- primarily \eiS{}.
  Given relation to No Feedback, may question, as No Feedback applies to bootstrapping, following from reliabislim.
  Hence, worry, as something sufficient to deny reliabilism is somewhat strong.

  Quick argument is that \nI{} is silent with respect to (at least some) instances of reasoning ruled out by anti-bootstrapping principles.
  Hence, \nI{} will not entail bootstrapping, and seems compatible with reliabilism.

  Focus on knowledge.
  However, justification also.
  Here, work with example from \textcite{Cohen:2010ux}, as this is designed for justification.\nolinebreak
  \footnote{
    Though, structurally similar to instance from \cite{Vogel:2000tl}.
  }

  \begin{quote}
    Suppose, having no idea whether my color vision is reliable, I decide to test it. I have someone stand across the room from me and hold up colored cards one at a time. I look at the first card and reason:
    \begin{enumerate}[label=(\arabic*)]
      \setcounter{enumi}{3}
    \item Card 1 looks red.
    \item Card 1 is red.
    \item Card 1 looks red and is red.
    \item So my color vision worked correctly.
    \end{enumerate}
    \[\vdots\]
    I reason the same way for each of the other cards: card 2 looks green, car 2 is green, so so card 2 looks green and is green, etc. I then infer

    \begin{enumerate}[label=(\arabic*), resume]
    \item My color vision worked correctly every time, i.e., I made no errors.
    \end{enumerate}

    Then I infer inductively that

    \begin{enumerate}[label=(\arabic*), resume]
    \item My color vision is reliable.\nolinebreak
      \mbox{}\hfill\mbox{(\citeyear[142]{Cohen:2010ux})}
    \end{enumerate}
  \end{quote}

  Two questions.
  Whether final step is \RBV{} and if so, whether support is included.

  Final step is induction.
  Plausible that it is.
  Issue, then, is whether inclusion.
  Induction is required, but unclear if not induction, so unclear that there's a plausible account of inclusion.

  If not by value, then the agent isn't required to go by value, and hence \nI{} doesn't apply.

  So, it seems \nI{} does not entail other bootstrapping principles.
  Of course, principles may hold.
  Still, \nI{} is sufficient rather than necessary.

  Simple upshot is that assumed principles about support seem quite weak.
\end{note}

\begin{note}[Wright on transmission failure]
  {
    \color{red}
    Given details on bootstrapping, may be best to leave \citeauthor{Wright:2011wn} to a later chapter.
  }
  The key idea behind \citeauthor{Wright:2011wn}'s account of transmission failure is whether something is a presupposition of a cognitive project.
  Failure when agent uses result of cognitive project to claim warrant for something that was presupposed.
  Hence, a kind of circularity.

  \nI{} is different.
  Does not require \(\psi\) to be a presupposition.
  Agent does not need to presuppose novel application of basis of support for some proposition in order to claim support for some other proposition --- or to be diligent, it is not clear to me that this is the case.

  To illustrate, different readings on Moore's proof.
  For \citeauthor{Wright:2011wn}, presupposition of cognitive project that sight is factive.
  So, this already gets that there's an external world.

  For \nI{}, problem if one thinks that appearance of hand also applies to existence of external world.

  Here, if follow \citeauthor{Wright:2011wn}, then support won't also extend, as presupposed.
  Alternatively, one may argue that the support is for world and hands.
  Hence, that support for world comes from a reapplication of the support used for hands.

  So, this distinguishes \nI{} from transmission failure.
  More is said in chapter~\ref{cha:inertia}.
\end{note}

\section{Two Different Ways of Claiming Support}
\label{sec:two-different-ways}

\begin{note}
  Here, two different ways of claiming support.
  \RBV{-} is used when stating \nI{}, so it may be helpful to clarify.
  \incl{} is not used anywhere, but it provides a useful contrast to \RBV{}.
\end{note}

\subsection{Inclusion}
\label{sec:inclusion-support}

\begin{note}[Inclusion of support]
  \begin{proposition}[Inclusion of support --- \incl{}]
    Support for \(\phi\) includes/requires support for \(\psi\).
    If, possible to reapply premises that establish \(\phi\) to establish \(\psi\) (without appealing to value of \(\phi\)).
  \end{proposition}
  Basic idea of \incl{} is reapplication.
  Hence, \incl{}\dots

  Without going by \(\phi\).
  This is not an additional condition, but clarification.
  For, given \eiS{}, agent did not need value of \(\phi\) in order to claim support for \(\phi\), and as agent is reapplying claimed support for \(\phi\), such reapplication does not require value of \(\phi\) either.

    \begin{figure}[H]
    \begin{subfigure}{.45\textwidth}
      \centering
      \begin{tikzpicture}[->, >=stealth', node distance=0cm, every text node part/.style={align=center}, scale=0.75]
        \node [] (1) at (-1,4) {};
        \node [] (2) at (-1,-1) {};
        \node [] (3) at (4,-1) {};
        \node [] (4) at (4,4) {};

        \node [] (a) at (1.5,3) {\(\{P/S\}\)};
        \node [] (b) at (1.5,0) {\(S(\phi)\)};
        \node [] (c) at (3,0) {};
        \node [] (d) at (3,3) {};

        \draw [->,-{Square[open]}, xshift=4] (a) to  node[right] {} (b);
        \draw [rounded corners=5pt, opacity=.1] (-.75,3.75) -- (-.75,-.75) -- (3.75,-.75) -- (3.75,3.75) -- cycle;
      \end{tikzpicture}
      \caption{Support for \(S(\phi)\) from premises/steps}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.45\textwidth}
      \centering
      \begin{tikzpicture}[->, >=stealth', node distance=0cm, every text node part/.style={align=center}, scale=0.75]
        \node [] (1) at (-1,4) {};
        \node [] (2) at (-1,-1) {};
        \node [] (3) at (4,-1) {};
        \node [] (4) at (4,4) {};

        \node [opacity=.33] (a) at (.25,3) {\(\{P/S\}\)};
        \node [opacity=.33] (b) at (.25,0) {\(S(\phi)\)};
        \node [] (lt) at (1.5,3.25) {\(\leadsto\)};
        \node [] (dv) at (1.5,3) {\hdashrule[0.5ex][c]{12pt}{.1pt}{}};
        \node [] (ss) at (1.5,2.75) {\(\supseteq\)};
        \node [] (c) at (2.75,0) {\(S(\psi)\)};
        \node [] (d) at (2.75,3) {\(\{P'/S'\}\)};

        \draw [->,-{Square[open]}, opacity=.33] (a) to  node[right] {} (b);
        \draw [->,-{Square[open]},] (d) to  node[right] {} (c);
        \draw [rounded corners=5pt, opacity=.1] (-.75,3.75) -- (-.75,-.75) -- (3.75,-.75) -- (3.75,3.75) -- cycle;
      \end{tikzpicture}
      \caption{Reapply premises/steps to claim support for \(\psi\)}
    \end{subfigure}
    \caption{\incl{} diagram}
  \end{figure}

  Examples:
  \begin{itemize}
  \item \(p \rightarrow r\), includes support for \(p \rightarrow (q \land r)\) with premises \(p \rightarrow q\) and \(q \rightarrow r\).
  \item More complex example of entailment.
  \item Combining support for novel entailments.
  \item If understand language, then parse: some sentence.
  \item Optimal route to some place, then stop at some intermediate point at some time.
  \item Calculation of area, then calculation of diagonal. (A little trickier, as diagonal is background, and it's only the reuse of measurements.)
  \end{itemize}
  In these cases, some consequence.
  However, it's not simply any consequence.
  Rather, there is some resource that the agent will have used which may be refined.

  Clear failures for arbitrary entailment.
  Things used for proving completeness do not necessarily ensure finite model property, though in certain cases they may.

  And, finally, cases of ability.
  Ability is quite natural here, as when looking at the agent doing something, so perform some action.

  \begin{quote}
    If you've done X, then in a position to do Y.
  \end{quote}

  Doesn't hold for many cases.
  Two examples:
  \begin{itemize}
  \item Knowledge, support that \(S\) knows \(p\) does not include support for \(p\).
  \item Going by a supplied conditional.
    E.g.\ If not in London then in Paris.
    Searched London, but this doesn't already establish that the person is in Paris, the person could be anywhere other than London.
  \end{itemize}
\end{note}

\begin{note}[\incl{} and \nI{}]
  So, with respect to \nI{}, \incl{} highlights tight relation between support for \(\phi\) and support for \(\psi\).

  Because focus is on inclusion, issue of \(\psi\) is something of an (indirect, partial) test on the claimed support for \(\phi\).\nolinebreak
  \footnote{
    Partial, because it doesn't say too much about mistaken or misleading support that allows the agent to claim support for too much.
    However, pair with `not claim support for \(\psi\)' type conditions.
  }
  This is why issue of support for \(\psi\) is significant.
\end{note}


\subsection{Reasoning-by-value}
\label{sec:reasoning-value}

\begin{note}[\RBV{}]
  Second point is reasoning-by-value.

  \begin{proposition}[Reasoning by value (\RBV{})]
    An agent \emph{reasons by value} if the agent moves from claimed support for propositions \(\phi_{i}\) to \(\phi_{i}\) having value \(v_{i}\), which constrains value of \(\psi\).
  \end{proposition}

  \RBV{} is common.
  Agent has claimed support for \(\phi\) having value \(v\), then reason about what follows from \(\phi\) having value \(v\).
  Distinguishing feature is that it is \(\phi\) having value \(v\) which constrains value of \(\phi\).
  Purpose of going by value is that claimed support for \(\phi\) may not be sufficient to provide constraint of value of \(\psi\) without value of \(\phi\).

  First, entailments sometimes require value.
  Example with knowledge.
  It is true that an agent knows that \(p\) only if \(p\) is true.
  So, agent knowing \(p\) constrains value of \(p\).

  Support for agent knowing \(p\) without independent support for \(p\).
  For example, novice and an expert.
  Novice in position to claim support for expertise of expert, but not in a position to reason to \(p\) independently of expert.
  Appeal to claim of support.
  Factivity.
  Factivity requires that the expert knows, not merely that the agent has claimed support that the expert knows.
  Claimed support won't do this alone, for claimed support doesn't get \(p\) without \(K_{E}p\), nor does it need to be the case that \(K_{E}p\) in order to claim support for \(p\).\nolinebreak
  \footnote{
    Decline to link support to attitudes, but for clearer intuition, consider belief.
    \(B(K_{E}p)\) gets to \(B(p)\), but simply believing \(K_{E}p\) isn't enough to get \(K_{E}p\) and hence \(p\).
    So, \(K_{E}p\) given belief, hence \(p\), resulting in \(B(p)\).
  }
  Probe and find issues.

  Second, following entailment, information provided to agents is often about values.
  Information provides to an officer worker by company secretary that if their manager is not in the London office today, they are in the Paris office.
  Secretary does not provide information other than constraints on location.
  Employee searches London office, doesn't find manager.
  So, claim support that manager is not in the London office, and with the secretary's information, claim support that the manager is in the Paris office.

  As secretary doesn't provide information other than constraints, need not in London office to go to Paris office.

  Similarly, condition~\ref{nI:received-info} is about value, though abstract stated.

  \begin{figure}[H]
    \begin{subfigure}{.45\textwidth}
      \centering
      \begin{tikzpicture}[->, >=stealth', node distance=0cm, every text node part/.style={align=center}, scale=0.75]
        \node [] (1) at (-1,4) {};
        \node [] (2) at (-1,-1) {};
        \node [] (3) at (4,-1) {};
        \node [] (4) at (4,4) {};

        \node [] (a) at (0,3) {\(S(\phi)\)};
        \node [] (b) at (0,0) {\(\phi\)};
        \node [] (c) at (3,0) {\(\psi\)};
        \node [] (d) at (3,3) {};

        \draw [->, xshift=4] (b) to  node[right] {} (c);
        \draw [rounded corners=5pt, opacity=.1] (-.75,3.75) -- (-.75,-.75) -- (3.75,-.75) -- (3.75,3.75) -- cycle;
      \end{tikzpicture}
      \caption{Support for \(S(\phi)\) and info}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.45\textwidth}
      \centering
      \begin{tikzpicture}[->, >=stealth', node distance=0cm, every text node part/.style={align=center}, scale=0.75]
        \node [] (1) at (-1,4) {};
        \node [] (2) at (-1,-1) {};
        \node [] (3) at (4,-1) {};
        \node [] (4) at (4,4) {};

        \node [] (a) at (0,3) {\(S(\phi)\)};
        \node [] (b) at (0,0) {\(\phi\)};
        \node [] (c) at (3,0) {\(\psi\)};
        \node [] (d) at (3,3) {};

        \draw [->] (b) to  node[right] {} (c);
        \draw [->, -{Square[open]}] (a) to  node[right] {} (b);
        \draw [rounded corners=5pt, opacity=.1] (-.75,3.75) -- (-.75,-.75) -- (3.75,-.75) -- (3.75,3.75) -- cycle;
      \end{tikzpicture}
      \caption{Move to value of \(\phi\)}
    \end{subfigure}

    \begin{subfigure}{.45\textwidth}
      \centering
      \begin{tikzpicture}[>=stealth', node distance=0cm, every text node part/.style={align=center}, scale=0.75]
        \node [] (1) at (-1,4) {};
        \node [] (2) at (-1,-1) {};
        \node [] (3) at (4,-1) {};
        \node [] (4) at (4,4) {};

        \node [] (a) at (0,3) {\(S(\phi)\)};
        \node [opacity=0.33] (b) at (0,0) {\(\phi\)};
        \node [] (c) at (3,0) {\(\psi\)};
        \node [] (d) at (3,3) {};

        \draw [->, opacity=0.33] (b) to  node[right] {} (c);
        \draw [->,-{Square[open]}, opacity=0.33] (a) to  node[right] {} (b);
        \draw [->,-{Square[open]}] (a) to  node[right] {} (c);
        \draw [rounded corners=5pt, opacity=.1] (-.75,3.75) -- (-.75,-.75) -- (3.75,-.75) -- (3.75,3.75) -- cycle;
      \end{tikzpicture}
      \caption{Given support, value of \(\psi\) is determined}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.45\textwidth}
      \centering
      \begin{tikzpicture}[>=stealth', node distance=0cm, every text node part/.style={align=center}, scale=0.75]
        \node [] (1) at (-1,4) {};
        \node [] (2) at (-1,-1) {};
        \node [] (3) at (4,-1) {};
        \node [] (4) at (4,4) {};

        \node [opacity=0.66] (a) at (0,3) {\(S(\phi)\)};
        \node [opacity=0.33] (b) at (0,0) {\(\phi\)};
        \node [opacity=0.66] (c) at (3,0) {\(\psi\)};
        \node [] (d) at (3,3) {\(S(\psi)\)};

        \draw [->, opacity=0.33] (b) to  node[right] {} (c);
        \draw [->,-{Square[open]}, opacity=0.33] (a) to  node[right] {} (b);

        \draw [rounded corners=10pt] (-.5,3.5) -- (-.5,-.5) -- (3.5,-.5) -- (3.5,.5) -- (.5,.5) -- (.5,3.5) -- cycle;
        \draw [->,-{Square[open]}] (.64,.64) to node[right] {} (d);

        \draw [rounded corners=5pt, opacity=.1] (-.75,3.75) -- (-.75,-.75) -- (3.75,-.75) -- (3.75,3.75) -- cycle;
      \end{tikzpicture}
      \caption{Support for \(\psi\) from constraint on \(\psi\) following from support for \(\phi\)}
    \end{subfigure}
    \caption{\RBV{} diagram}
  \end{figure}
\end{note}


\begin{note}[Non-\RBV{} reasoning]
  Not all reasoning is like this.

  Return to examples used to illustrate inclusion of support.
  \begin{itemize}
  \item Conjunction elimination.
    Here, working with the logical form, it doesn't matter what \(p\), \(q\), nor \(r\) are.
  \item Same for efficient route example.
    Agent doesn't need to appeal to having found the most efficient route to produce variation.
  \end{itemize}

  Some instance of reasoning may go either way.
  For example.
  Machine.
  Told that the machine is designed to perform some function, but not told what the function is.

  Question, \(f(X,Y)\)

  One option is to input into machine.
  So, relying on machine implementing function.
  Going by value.

  Second option, abstract function from machine by inspection.
  Then, calculate \(f(X,Y)\).
  Not going by value, doesn't matter what the machine produced.

  Risks for each.
  For the first, potential malfunction on specific input, but without malfunction then that's the result.
  For the second, no concern about potential malfunction, but possible that abstracted function is faulty.

  So, difference is not tied to proposition.

  And, complex chain of reasoning may include both.
  \(p \rightarrow (q \land K_{S}r)\) so \(p \rightarrow q \land r\).
\end{note}


\section{Difficult Cases}
\label{sec:difficult-cases}


\begin{note}[General form of difficult cases]
  Generate difficult cases in a systematic way.
  Roughly, any time someone implements something that is a shortcut to other information.
  Return to calculator illustration.
  Something puzzling about the technician.
  However, this is quite common.

  Bookmark.
  Easy to check that this is how far I've got in the book.
  If not, then the bookmark is no good.

  Doorbell, but in a position to check whether there is someone at the door.

  Requests for a colleague to perform some task.
  I.e.\ spreadsheets.

  The key point with all of these examples is that the shortcut is only good if you think that the relevant \(\psi\) instance is the case.
  So, breaks the `\eit{}'.

  However, this goes to fast.
  What we're picking up is not that \(\psi\) needs to be the case --- that much is clear from \ref{nI:claimed-support} --- but that the agent is in a position to claim support for \(\psi\) without appeal to value of \(\phi\).
  The issue is not that \(\psi\) is required to be the case, such a result is the use of reasoning by value.
  No, the issue is with whether the claimed support for \(\phi\) really is any good.

  Hence, the problem highlighted here is roughly that things need to function as expected in order for shortcuts to be of any use.
  Still, there is a difference between \(\phi\) and \(\psi\) working together for the shortcut, and appealing to \(\phi\) and the shortcut requiring \(\psi\) to observe that \(\psi\) is the case and hence you were fine to appeal to the shortcut.

  The latter is intuitively problematic.
  Lots of things going on.
  Here, suggestion is that at least one, sufficient for failure, is that we've got no idea whether the shortcut works.
  It might, but we're appealing to success in order to argue that it does.
  This is what is repeated and expanded on.
  Issue is not that \(\psi\) is required by \(\phi\).
  Rather, that \(\psi\) is required in order to the agent to appeal to claimed support.
  Observing that \(\psi\) follows is then of no real interest, because if not \(\psi\) then no idea about \(\phi\).

  This doesn't fully resolve the query, though.
  We've shown that shortcuts are plausibly OK when \ref{nI:inclusion} doesn't hold.
  However, many cases in which it does seem to hold, and the shortcut seems to be fine.
  The shortcut is there because it's reduces effort.
  And, with a little more effort, one could check the \(\psi\) condition.

  Easy to deal with in the case of testimony.
  Conditional is truthful then something mundane.
  Here, if the \(\psi\) condition doesn't hold, then it's really not clear that one gets a problem with the claimed support for testimony.
  The issue is very local, and claim for testimony would need to be unreasonably strong.
  And, conversely, does seem problematic if the testimony claim is extremely narrow.
  So, the point is that there's no plausible instance of \ref{nI:inclusion} for testimony case, even though the \(\psi\) condition is true.

  So, \nI{} applies to these one-off cases, or when there is going to be some significant problem from a single point of failure.

  Still, the problem identified by \nI{} differs slightly.
  Latter is intuitively problematic because using the result of an assumption to discharge the use of the assumption.
  \nI{} doesn't require assuming claimed support for \(\psi\).
  Rather, 

  Corollary is that one needs to expand as much as possible when one can.
  Commendable, but does not seem required for claiming support.
\end{note}