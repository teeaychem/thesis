%%% Local Variables:
%%% TeX-master: "master"
%%% End:


\chapter{Inertia}
\label{cha:inertia}

\section{Inertia}
\label{sec:inertia}

Argument against~\ref{denied-claim} is that it conflicts with \nI{-} in scenarios highlighted by~\ref{prem:ab}.

\begin{enumerate}[label=\nI{}, ref=\nI{}]
\item An agent is not able to obtain support for some proposition \(\psi\) on the basis of information that some the support the agent has for \(\phi\) is misleading or mistaken if \(\psi\) is not the case.
\end{enumerate}

There is lack of an explanatory connexion between support for \(\phi\) and support for \(\psi\).

\section{nI}
\label{sec:ni}

Case in which the support the agent has does not trace from access, or the agent obtains support on the basis that the support the agent does have is misleading.

Note, talk about `obtaining' support.
So, we have a setup where:

\begin{enumerate}
\item Agent has support.
\item Say, for some proposition \(\psi\).
\item The support for \(\psi\) is not, given the agent's doxastic state, also support for \(\psi\).
\item Agent receives information to the extent that the agent's support for \(\psi\) is misleading if \(\psi\) is not the case.
\item Agent holds that the support for \(\psi\) supports \(\phi\) being the case, because else their support is misleading.
\end{enumerate}

Here, the support doesn't do anything for \(\phi\).

This principle is less obvious, but I think fine.

Consider a few good cases and some bad cases.

Good case, deduction.
So, if support for \(\phi\) then support for \(\phi \lor \psi\), because, intuitively, support for \(\phi\) is also support for \(\phi \lor \psi\).
It is puzzling to motivate on the basis that the agent obtains support for \(\phi \lor \psi\) because if \(\phi \lor \psi\) is not the case then their support for \(\phi\) would be misleading.
Of course, this is the case.
However, that is not why the agent has support for \(\phi \lor \psi\).


Suppose quiz.
Interested in answer to a certain question, mix with questions I know the answer to.
Contestant gets the answers correct.
Support that the contestant is good with respect to the domain.
Have support for the question I'm interested in.

Problem if that I hold the answer to the question is such and such because otherwise support for contestant understanding the domain would be misleading.

These are two cases where things look straightforward, and the agent has support by some other route.

If X then support is misleading
Therefore, not-X.

???

If that's not a boat then the radar is faulty.

Have support that the radar is working well.
So, it is a boat.

??? Though in this case it looks as though there's a way to go from functioning radar to boat.

Slight variant.
Checked the radar, support that it's functioning correctly.
Not looking at the screen.

If there's a blip on the radar, then it's not functioning correctly.
Support that it's functioning correct.
So, there's no blip on the radar.



Parked car outside.
If car is stolen then support that this is a safe neighbourhood would be misleading.
Support for car is not stolen.


If the seeds haven't started sprouting yet, then you've been misled about the planting conditions.
Have not been misled about the planting conditions.
So, the seeds have started sprouting.

\begin{note}[Denying~\nI{}]
  Worry about coherence.

  If the agent does not obtain support, then in a situation where:
  \begin{itemize}
  \item Support for \(\phi\)
  \item Information that if \(\phi\) is the case then \(\psi\) is the case.
  \item No support for \(\psi\).
  \end{itemize}

  So, agent is committed in some sense to \(\psi\) being the case, given the support they have for \(\phi\) and the information.

  I have support for \(\phi\) and information that \(\phi \rightarrow \psi\), but no support for \(\psi\).

  Looks like a failure of closure.

  However, closure is typically formulated with entailment.
  In each of these cases, there's no entailment.
  Distinction between the support for \(\phi\) and \(\phi\).

  Would need something to the effect of positive attitude only if support.

  However,~\nI{} only denies support.
  It does not deny that the agent is require to have some positive attitude toward the proposition.
\end{note}

\begin{note}[Undercutting defeaters]
   Undercutting defeaters are of interest with respect to~\nI{} is that (merely) suggesting that the agent's support for \(\phi\) is no good can amount to highlighting that the agent's support for \(\phi\) may be misleading.

  Consider the following illustration provided by \citeauthor{Pollock:1987un}\nolinebreak
  \footnote{
    \citeauthor{Pollock:1987un} defines an undercutting defeater as follows:
    \begin{quote}
      R is an \emph{undercutting defeater} for P as a prima facie reason for S to believe Q if and only if
      \begin{enumerate}[label=(UD\arabic*), ref=(UD\arabic*)]
      \item P is a reason for S to believe Q and R is logically consistent with P but (P and R) is not a reason for S to believe Q, and
      \item R is a reason for denying that P wouldn't be true unless Q were true.\nolinebreak
        \mbox{}\hfill\mbox{(\citeyear[485]{Pollock:1987un})}
      \end{enumerate}
    \end{quote}
    Intuitively, an undercutting defeater for P as a reason for Q because it the defeater denies that Q must be true in order for P to be true.
  }\nolinebreak
  :
  \begin{quote}
    [Undercutting defeaters] attack the connection between the reason and the conclusion rather than attacking the conclusion itself.
    For instance, ``X looks red to me'' is a prima facie reason for me to believe that X is red.
    Suppose I discover that X is illuminated by red lights and illumination by red lights often makes things look red when they are not.
    This is a defeater, but it is not a reason for denying that X is red (red things look red in red light too).
    Instead, this is a reason for denying that X wouldn't look red to me unless it were red.\nolinebreak
    \mbox{}\hfill\mbox{(\citeyear[485]{Pollock:1987un})}
  \end{quote}
  Completing \citeauthor{Pollock:1987un}'s example, it seems that if agent's support for holding that X is red is that `X wouldn't look red to me unless it were red', then the support for X being red provided by appearance is retracted after discovering that X is illuminated by red lights (though it remains possible that X is red).

  Assumption --- in violation of \nI{} --- an agent always has the option to obtain support for \(\psi\) on the basis of information that the support the agent has for \(\phi\) is misleading if \(\psi\) is not the case.

  Given that it remains possible that X is red, it follows that the appearance of X being red would be misleading if X is not red.
  For, it continues to be the case that X appears to be red, even after the discovery is made by the agent.
  Therefore, by the assumption violating~\nI{}, support for X being red persists in the presence of the undercutting defeater.
  Still, intuitively it seems that the undercutting defeater of a red light blocks the agent obtaining support for X being red on the basis of their visual perception.

  This is partial motivation for~\nI{}, as substituted `does not have the option' from the text of~\nI{} with `always has the option' when making the problematic assumption.
  Further argument is (hopefully) to follow.

\end{note}


\section{Motivation for \nI{}}
\label{sec:motivation-ni}

\begin{itemize}
\item Because, the support the agent has is independent of \(A(\psi)\)/\(\psi\).
\end{itemize}

\begin{itemize}
\item An agent is not able to obtain support for some proposition \(\psi\) on the basis of information that the support the agent has for \(\phi\) is misleading if \(\psi\) is not the case.
\end{itemize}

\begin{itemize}
\item The relevant information must also provide some support for \(\phi\).
\item One way of getting to this is by ordering support.
\item If the constraint is established prior to obtaining support, then this may limit support.
\end{itemize}

\begin{itemize}
\item This is also related to Harman.
\item For, the principle there is that one is not in a position to hold that support is going to be misleading.
\item Support for \(\phi\) does not show that future support for \(\psi\) is misleading when \(\psi \vdash \lnot\phi\).
\item Support for \(\phi\) does not show that\dots
\end{itemize}

\begin{itemize}
\item Important to note is that this does not deny closure.
\item First, doxastic.
\item Second, no requirement that the required information is a (known, logical) entailment.
\end{itemize}

Why does witnessing work?

\begin{itemize}
\item Because the agent is not basing things on the support they have.
\item The things about misleading support is that it doesn't say there's anything problematic about the information received.
\end{itemize}

\subsection{Motivation, taken from overview}
\label{sec:motiv-taken-from}

\begin{note}[Some motivation for \nI{}, 1]
  Some motivation for~\nI{} may be found by constructing instances of reasoning which violate~\nI{}.
  A couple of cases may help.

  For example, suppose our agent is out shopping for a gift for Sam with a friend and has not yet considered the items before them.
  The friend remarks that `Someone would only buy \emph{that} (some particular item) as a gift for Sam if they didn't know Sam very well.'
  The agent has support that they do know Sam very well.
  However, it does not seem permissible for the agent to obtains support that they wouldn't buy that as a gift, one the basis of the support they have their familiarity with Sam and the statement made by their friend.
  For, it is possible that they agent would have settled on the particular item if they had given it some consideration --- the support the agent has for their familiarity with Sam \emph{may} be misleading.

  Or, suppose the agent has parked their car on the street outside of Sam's place and reasons that:
  If my car is stolen then my support that this is a safe neighbourhood would be misleading.
  Therefore, as my support that this is a safe neighbourhood is not misleading, my car has not been stolen.
  The agent's reasoning seems confused.
  Perhaps the agent has the option of extending the support the have that the neighbourhood is safe to obtain support that they car remains parked outside, but that support the agent has would be misleading if their car has been stolen does not seem appropriate.
\end{note}

\begin{note}[Some motivation for \nI{}, 2]
  For further motivation, consider undercutting defeaters.

  We take the following sketch from \textcite{Worsnip:2018aa}:
  \begin{quote}
    Undercutting defeaters, which are easiest to think of in the context of the attitude of belief, are supposed to be considerations that undermine the justification of a belief in a proposition p not necessarily by providing (sufficient) positive evidence to think that p is false, but rather merely by suggesting (perhaps misleadingly) that oneâ€™s reasons for believing p are no good, in a way that neutralizes or mitigates their justificatory or evidential force.\nolinebreak
    \mbox{}\hfill\mbox{(\citeyear[29]{Worsnip:2018aa})}
  \end{quote}
\end{note}


\subsection{Misleading support}
\label{sec:misleading-support}

\begin{note}[Misleading, again]
  By the support that the agent has for some proposition being \emph{misleading} we mean that the support the agent has for \(\phi\) provides good reason for thinking that \(\phi\) is the case, even if \(\phi\) is not the case.\nolinebreak
  \footnote{
    Different from `mistaken', where \(\phi\) turns out to be true, but the support itself provides the wrong understanding of why \(\phi\) is true.

    Suspect that a variant of \nI{} holds for `mistaken'.
    However, `misleading' is sufficient.
  }
  For example, see someone entering a car by unlocking the drivers door with a coat-hanger.
  I take this to be good support for the proposition that the person is not the owner of the car.
  As it turns out, the person had accidentally locked their only set of keys in the car, and did not want to call a locksmith.
  The support for the proposition that the person is not the owner of the car was misleading, not only because the person owns the car, but because it continues to provide support even with the knowledge that the person is the owner.
  It would be clear to the owner why they would be stopped by a police officer, and why the owner is required to provide documentation to show that they are the owner (i.e.\ support to the contrary proposition).

  In this sense, the support is misleading.
  The support is `good', even though the proposition is false.

  `Misleading' support is distinct from an error made by the agent, such as taking the use of the coat-hanger to be support for the proposition that the someone had stolen the person's coat and locked the coat inside the car.\nolinebreak
  \footnote{
    No doubt there is additional background that could be added\dots
  }
  And, misleading support is distinct from `mistaken' in which an agent obtain support for some proposition which is true but where some part of the support is false.

  For example, person using the coat-hanger looks like the owner, but the car had recently been sold.
  In contrast to error, there seems no problem with the agent holding a positive attitude toward the proposition given mistaken and misleading support, the relevant difference is whether the proposition does hold.

  Intuitively, then, the trouble with misleading support is that it suggests, but does not ensure that the proposition holds.

  Misleading support, then, is not problematic.
\end{note}


\begin{note}[Combination]
  \nI{} and \nIm{} combine:

  \begin{proposition}[\nIp{}]
    An agent does not have the option of claiming support for some proposition \(\xi\) from observing that \(\psi\) entails \(\xi\) and information that the support the agent claims for \(\phi\) is misleading if \(\psi\) is not the case.
  \end{proposition}

  With respect to scenarios of interest.
    \(\phi = \) general ability.
    \(\psi = \) specific ability.
    \(\xi = \) strategy exists.

    So, application of \nI{} (and \nIm{}) is that the agent does not have option of obtaining support for specific ability from information that the support they have for general ability is misleading if the agent does not have the specific ability.

    Stated independently, \nIp{} is a weaker proposition than \nI{} and \nIm{}.\nolinebreak
    \footnote{
      as \nIp{} makes no statement about whether the agent may claim support for \(\psi\) (though \nI{} does).

      From \nI{}, the agent does not get support for \(\psi\), and by \nIm{} agent requires support for \(\psi\) to get support for \(\xi\), and hence does not obtain support for \(\xi\).

    Still, as it is the result of interest, some room to pursue the present line of argument if either \nI{} or \nIm{} is rejected.
    }
    Suspect that any argument from \nIp{} follows from motivation for \nI{} and \nIm{}.

  If \nI{} (and \nIm{}) hold, then trouble if support for specific ability comes from general being misleading if general is not the case.
\end{note}


\begin{note}[Relation to transmission]
  Here, difference to failure of transmission.
  In those cases, agent knows, hence \(\phi\) not being true is not much of an issue.
  In turn, \(\psi\).

  For us, \(\phi\) may be false.

  Whether there is a proper distinction, though, isn't quite so easy.
  Dogmatism gets thrown into the mix.
\end{note}


\begin{note}[Things not claimed]
  Don't appeal to possibility that support for \(\psi\) will be stronger for \(\psi\) having different value.
  Support isn't sufficiently strong.\nolinebreak
  \footnote{
    This requirement distinguishes from transmission failure.
  }
  Given that \(\psi\) is a dependency, \(\psi\) may be why strength of support is limited.
  And, from the above, lack of use of support for \(\psi\), hence agent is not in a position to go either way.
\end{note}


\section{Relatated literature}
\label{sec:relatated-literature}

\begin{note}[Related to\dots]
  Focus is on support.
  A pair of related things to illustrate interesting parts of \nI{}.
\end{note}

\begin{note}[No feedback/Bootstrapping]
  No feedback by \citeauthor{Weisberg:2010to}.

  \begin{quote}
    \textbf{No Feedback} If
    \begin{enumerate*}[label=(\roman*), ref=(\roman*)]
    \item\label{WB:NF:1} \(L_{1}-L_{n}\) are inferred from \(P_{1}-P_{m}\), and
    \item\label{WB:NF:2} \(C\) is inferred from \(L_{1}-L_{n}\) (and possibly some of \(P_{1}-P_{m}\) by an argument whose justificatory power depends on making \(C\) at least \(x\) probable, and
    \item\label{WB:NF:3} \(P_{1}-P_{m}\) do not make \(C\) at least \(x\) probable without the help of \(L_{1}-L_{n}\), then the argument for \(C\) is defeated.\nolinebreak
      \mbox{}\hfill\mbox{(\citeyear[533--534]{Weisberg:2010to})}
    \end{enumerate*}
  \end{quote}

  Some similarities.
  Stated in probabilistic terms, so no easy application.
  And, addition of \(C\) condition.
  Expand to add this.

  Somewhat different.
  For, inclusion of support, which (at least without further work) is qualitative rather than quantitative.
  So, it seems \ref{WB:NF:3} isn't going to hold.

  Still, reform No Feedback a little.
  Consider \ref{WB:NF:3} from the agent's point of view, independent of whether \(P_{1}-P_{m}\) not make \(C\) at least \(x\) probable.
  Plausibly problematic if agent doesn't have response to~\ref{WB:NF:3}.
  And, this would be issue of not showing how \(P_{1}-P_{m}\) relate to \(L_{1}-L_{n}\).
\end{note}

\begin{note}[Bootstrapping and reliabilism]
  \nI{} is distinct from No Feedback, but shares some similarities.
  Attempted to keep to weak principles regarding support --- primarily \eiS{}.
  Given relation to No Feedback, may question, as No Feedback applies to bootstrapping, following from reliabislim.
  Hence, worry, as something sufficient to deny reliabilism is somewhat strong.

  Quick argument is that \nI{} is silent with respect to (at least some) instances of reasoning ruled out by anti-bootstrapping principles.
  Hence, \nI{} will not entail bootstrapping, and seems compatible with reliabilism.

  Focus on knowledge.
  However, justification also.
  Here, work with example from \textcite{Cohen:2010ux}, as this is designed for justification.\nolinebreak
  \footnote{
    Though, structurally similar to instance from \cite{Vogel:2000tl}.
  }

  \begin{quote}
    Suppose, having no idea whether my color vision is reliable, I decide to test it. I have someone stand across the room from me and hold up colored cards one at a time. I look at the first card and reason:
    \begin{enumerate}[label=(\arabic*)]
      \setcounter{enumi}{3}
    \item Card 1 looks red.
    \item Card 1 is red.
    \item Card 1 looks red and is red.
    \item So my color vision worked correctly.
    \end{enumerate}
    \[\vdots\]
    I reason the same way for each of the other cards: card 2 looks green, car 2 is green, so so card 2 looks green and is green, etc. I then infer

    \begin{enumerate}[label=(\arabic*), resume]
    \item My color vision worked correctly every time, i.e., I made no errors.
    \end{enumerate}

    Then I infer inductively that

    \begin{enumerate}[label=(\arabic*), resume]
    \item My color vision is reliable.\nolinebreak
      \mbox{}\hfill\mbox{(\citeyear[142]{Cohen:2010ux})}
    \end{enumerate}
  \end{quote}

  Two questions.
  Whether final step is \RBV{} and if so, whether support is included.

  Final step is induction.
  Plausible that it is.
  Issue, then, is whether inclusion.
  Induction is required, but unclear if not induction, so unclear that there's a plausible account of inclusion.

  If not by value, then the agent isn't required to go by value, and hence \nI{} doesn't apply.

  So, it seems \nI{} does not entail other bootstrapping principles.
  Of course, principles may hold.
  Still, \nI{} is sufficient rather than necessary.

  Simple upshot is that assumed principles about support seem quite weak.
\end{note}

\begin{note}[Wright on transmission failure]
  {
    \color{red}
    Given details on bootstrapping, may be best to leave \citeauthor{Wright:2011wn} to a later chapter.
  }
  The key idea behind \citeauthor{Wright:2011wn}'s account of transmission failure is whether something is a presupposition of a cognitive project.
  Failure when agent uses result of cognitive project to claim warrant for something that was presupposed.
  Hence, a kind of circularity.

  \nI{} is different.
  Does not require \(\psi\) to be a presupposition.
  Agent does not need to presuppose novel application of basis of support for some proposition in order to claim support for some other proposition --- or to be diligent, it is not clear to me that this is the case.

  To illustrate, different readings on Moore's proof.
  For \citeauthor{Wright:2011wn}, presupposition of cognitive project that sight is factive.
  So, this already gets that there's an external world.

  For \nI{}, problem if one thinks that appearance of hand also applies to existence of external world.

  Here, if follow \citeauthor{Wright:2011wn}, then support won't also extend, as presupposed.
  Alternatively, one may argue that the support is for world and hands.
  Hence, that support for world comes from a reapplication of the support used for hands.

  So, this distinguishes \nI{} from transmission failure.
  More is said in chapter~\ref{cha:inertia}.
\end{note}
